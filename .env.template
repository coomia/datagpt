###########################################################
#                   DATABASE CONFIG                      #
###########################################################
# Database driver: postgresql is recommended
DB_DRIVER=postgresql

# Database host, use service name if running in Docker Compose
DB_HOST=postgres

# Database credentials
DB_USER=postgres
DB_PASS=postgresql123
DB_PORT=5432
DB_NAME=datagpt

###########################################################
#                      REDIS CONFIG                       #
###########################################################
# Redis host, use service name in Docker Compose
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
# Optional Redis password
REDIS_PASSWORD=

###########################################################
#                      TRINO CONFIG                       #
###########################################################
# Trino user and connection info
TRINO_USER=admin
TRINO_HOST=trino
TRINO_PORT=8080
TRINO_CATALOG=postgresql
TRINO_SCHEMA=public

###########################################################
#                  VECTOR DATABASE CONFIG                 #
###########################################################
# Default vector DB is pgvector (embedded in PostgreSQL)
VECTOR_DB=pgvector
VECTOR_COLLECTION=schema_info

# If you want to use Qdrant instead, uncomment and configure:
# VECTOR_DB=qdrant
# QDRANT_HOST=qdrant
# QDRANT_PORT=6333

###########################################################
#                   EMBEDDING MODEL CONFIG                #
###########################################################
# Default embedding model is Ollama
EMBEDDING_MODEL_TYPE=ollama
EMBEDDING_MODEL_NAME=nomic-embed-text
EMBEDDING_MODEL_DIMENSIONS=768
OLLAMA_HOST=http://ollama-service:11434

# If you want to use OpenAI embeddings:
# EMBEDDING_MODEL_TYPE=openai
# EMBEDDING_MODEL_NAME=text-embedding-3-small
# OPENAI_API_KEY=your_openai_api_key

###########################################################
#                     LARGE LLM CONFIG                    #
###########################################################
# For OpenAI LLM (recommended for general users):
# MODEL_ID=gpt-4o-mini
# OPENAI_API_KEY=your_openai_api_key

# For Qwen / DashScope (enterprise users, fast inference in China):
# MODEL_ID=qwen-flash-2025-07-28
# DASHSCOPE_API_KEY=your_dashscope_api_key

# For Deepseek (heavy analytics, slower but capable):
# MODEL_ID=deepseek-chat
# DEEPSEEK_API_KEY=your_deepseek_api_key

# For KIMI / Moonshot models:
# MODEL_ID=moonshot-v1-32k
# MOONSHOT_API_KEY=your_moonshot_api_key

###########################################################
#                      NOTES & RECOMMENDATIONS           #
###########################################################
# 1. Users in China may prefer Qwen or KIMI for lower latency.
# 2. Users with OpenAI accounts are recommended to use OpenAI embeddings 
#    (text-embedding-3-small) and LLM gpt-4o-mini or gpt-4o for balance of speed and quality.
# 3. To switch vector DB to Qdrant, configure VECTOR_DB and QDRANT_HOST/PORT accordingly.
# 4. Keep all API keys and credentials secret; do NOT commit them to Git.
