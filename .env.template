###########################################################
#                     DATABASE CONFIG                      #
###########################################################
# Primary metadata database for DataGPT
# Recommended: PostgreSQL (best support + pgvector)
DB_DRIVER=postgresql
DB_HOST=postgres
DB_USER=postgres
DB_PASS=postgresql123
DB_PORT=5432
DB_NAME=datagpt


###########################################################
#                         CACHE CONFIG                    #
###########################################################
# Cache backend: "memory" (default) or "redis"
# CACHE_TYPE=memory

# If you want to use Redis, uncomment these:
CACHE_TYPE=redis
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=123456    # Optional


###########################################################
#                   DATA SOURCE CONFIG                    #
###########################################################
# Database to search for data and analysis
# Supported: postgresql / mysql / trino / clickhouse / doris
SEARCH_DB_TYPE=clickhouse
SEARCH_DB_HOST=clickhouse
SEARCH_DB_PORT=8123
SEARCH_DB_NAME=olist
SEARCH_DB_USER=olist
SEARCH_DB_PASSWORD=olist123456

###########################################################
#                   DATA QUALITY EXECUTION ENGINE         #
###########################################################
TRINO_USER=admin
TRINO_HOST=trino
TRINO_PORT=8080
TRINO_CATALOG=clickhouse  # 比如 mysql, postgresql, hive
TRINO_SCHEMA=olist


###########################################################
#                 VECTOR DATABASE CONFIG                  #
###########################################################
# Default vector store is pgvector (embedded in PostgreSQL)
VECTOR_DB=pgvector
VECTOR_COLLECTION=schema_info

# To use Qdrant instead, uncomment:
# VECTOR_DB=qdrant
# QDRANT_HOST=qdrant
# QDRANT_PORT=6333

# Minimum interval (in seconds) between two vector reindex operations, in order to detect schema changes, default 86400 (24 hours)
#VECTOR_REINDEX_INTERVAL_SECONDS=86400


###########################################################
#                EMBEDDING MODEL CONFIG                   #
###########################################################
# Embedding model provider:
# - ollama   → local inference (fastest, no token cost)
# - openai   → best quality, recommended for production
EMBEDDING_MODEL_TYPE=ollama
EMBEDDING_MODEL_NAME=nomic-embed-text
EMBEDDING_MODEL_DIMENSIONS=768
OLLAMA_HOST=http://ollama-service:11434

# OpenAI embeddings example:
# EMBEDDING_MODEL_TYPE=openai
# EMBEDDING_MODEL_NAME=text-embedding-3-small
# OPENAI_API_KEY=your_openai_api_key



###########################################################
#                     LARGE LLM CONFIG                    #
###########################################################
# Supported LLM providers:
#   openai     → OpenAIChat
#   claude     → Claude / Anthropic
#   anthropic  → Claude
#   deepseek   → DeepSeek
#   qwen       → DashScope (阿里)
#   kimi       → Kimi
#   doubao     → Doubao (字节)
#   google     → Gemini
#   meta       → LlamaOpenAI
#   xai        → xAI (Grok)

# Recommended selections:
# - China mainland: qwen / kimi / doubao (低延迟)
# - Global users: openai / claude / deepseek (高质量)
# - Local inference: meta (Llama via OpenAI-compatible server)

MODEL_PROVIDER=qwen
MODEL_ID=qwen-flash
MODEL_API_KEY=your_api_key
#MODEL_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1 # Optional, for your own compatible API server


###########################################################
#                      API SERVER CONFIG                  #
###########################################################

AGENT_DEBUG_MODE=true


