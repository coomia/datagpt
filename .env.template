###########################################################
#                     DATABASE CONFIG                      #
###########################################################
# Primary metadata database for DataGPT
# Recommended: PostgreSQL (best support + pgvector)
DB_DRIVER=postgresql
DB_HOST=postgres
DB_USER=postgres
DB_PASS=postgresql123
DB_PORT=5432
DB_NAME=datagpt


###########################################################
#                         CACHE CONFIG                    #
###########################################################
# Cache backend: "memory" (default) or "redis"
CACHE_TYPE=memory

# If you want to use Redis, uncomment these:
# CACHE_TYPE=redis
# REDIS_HOST=redis
# REDIS_PORT=6379
# REDIS_DB=0
# REDIS_PASSWORD=123456    # Optional


###########################################################
#                   DATA SOURCE CONFIG                    #
###########################################################
# Database to search for data and analysis
# Supported: postgresql / mysql / trino / clickhouse / doris
SEARCH_DB_TYPE=postgresql
SEARCH_DB_HOST=postgres
SEARCH_DB_PORT=5432
SEARCH_DB_USER=postgres
SEARCH_DB_PASSWORD=postgresql123
SEARCH_DB_SCHEMA=public
SEARCH_DB_NAME=datagpt


###########################################################
#                 VECTOR DATABASE CONFIG                  #
###########################################################
# Default vector store is pgvector (embedded in PostgreSQL)
VECTOR_DB=pgvector
VECTOR_COLLECTION=schema_info

# To use Qdrant instead, uncomment:
# VECTOR_DB=qdrant
# QDRANT_HOST=qdrant
# QDRANT_PORT=6333


###########################################################
#                EMBEDDING MODEL CONFIG                   #
###########################################################
# Embedding model provider:
# - ollama   → local inference (fastest, no token cost)
# - openai   → best quality, recommended for production
EMBEDDING_MODEL_TYPE=ollama
EMBEDDING_MODEL_NAME=nomic-embed-text
EMBEDDING_MODEL_DIMENSIONS=768
OLLAMA_HOST=http://ollama-service:11434

# OpenAI embeddings example:
# EMBEDDING_MODEL_TYPE=openai
# EMBEDDING_MODEL_NAME=text-embedding-3-small
# OPENAI_API_KEY=your_openai_api_key


###########################################################
#                     LARGE LLM CONFIG                    #
###########################################################
# Supported LLM providers:
#   openai     → OpenAIChat
#   claude     → Claude / Anthropic
#   anthropic  → Claude
#   deepseek   → DeepSeek
#   qwen       → DashScope (阿里)
#   kimi       → Kimi
#   doubao     → Doubao (字节)
#   google     → Gemini
#   meta       → LlamaOpenAI
#   xai        → xAI (Grok)

# Recommended selections:
# - China mainland: qwen / kimi / doubao (低延迟)
# - Global users: openai / claude / deepseek (高质量)
# - Local inference: meta (Llama via OpenAI-compatible server)

MODEL_PROVIDER=qwen
MODEL_ID=qwen-flash-2025-07-28
MODEL_API_KEY=your_api_key_here
#MODEL_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1


###########################################################
#                      API SERVER CONFIG                  #
###########################################################
API_PORT=8000
